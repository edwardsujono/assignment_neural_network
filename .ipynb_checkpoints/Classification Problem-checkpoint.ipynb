{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Design a 3-layer feedforward neural network consisting of a hidden-layer of 10 neurons having logistic activation function and an output softmax layer. Assume a learning rate ð›¼ = 0.01 and decay parameter ð›½ = 10âˆ’6. Use appropriate scaling of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "# by specifying [10] as the hidden_layer_neuron implies using 1 hidden layer with 10 neurons\n",
    "# respectively by specifying [100, 100] -> 2 hidden layers each layer 100 neurons\n",
    "\n",
    "\n",
    "class SoftmaxNeuralNetwork:\n",
    "\n",
    "    def __init__(self, train_x, train_y, num_features=6, list_of_neuron_on_hidden_layer=list([10]), decay=1e-6):\n",
    "\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "\n",
    "        self.train_cost = []\n",
    "        self.train_prediction = []\n",
    "\n",
    "        self.test_cost = []\n",
    "        self.test_prediction = []\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        # first layer which connect to the input layer\n",
    "        weights.append(\n",
    "            self.init_weight(len(train_x[0]), list_of_neuron_on_hidden_layer[0]) )\n",
    "        biases.append(\n",
    "            self.init_bias(list_of_neuron_on_hidden_layer[0]))\n",
    "\n",
    "        previous_layer = list_of_neuron_on_hidden_layer[0]\n",
    "\n",
    "        for layer in range(1, len(list_of_neuron_on_hidden_layer)):\n",
    "            weights.append(\n",
    "                self.init_weight(previous_layer, list_of_neuron_on_hidden_layer[layer]))\n",
    "\n",
    "            biases.append(\n",
    "                self.init_bias(list_of_neuron_on_hidden_layer[layer]))\n",
    "\n",
    "        # for output layer\n",
    "        weights.append(\n",
    "            self.init_weight(previous_layer, num_features)\n",
    "        )\n",
    "\n",
    "        biases.append(\n",
    "            self.init_bias(num_features)\n",
    "        )\n",
    "\n",
    "        # construct neural network\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        x_input = T.matrix('X')\n",
    "        y_output = T.matrix('Y')\n",
    "\n",
    "        prev_input = x_input\n",
    "\n",
    "        for i in range(len(weights)-1):\n",
    "            calculation = T.nnet.sigmoid(T.dot(prev_input, weights[i]) + biases[i])\n",
    "            layers.append(calculation)\n",
    "            prev_input = calculation\n",
    "\n",
    "        # last output layer, use softmax function\n",
    "        calculation = T.nnet.softmax(T.dot(prev_input, weights[len(weights)-1]) +\n",
    "                                     biases[len(biases) - 1])\n",
    "        layers.append(calculation)\n",
    "\n",
    "        y_prediction = T.argmax(calculation, axis=1)\n",
    "\n",
    "        sum_sqr_weights = T.sqr(weights[0])\n",
    "\n",
    "        for i in range(1, len(weights)):\n",
    "            sum_sqr_weights += T.sum(T.sqr(weights[i]))\n",
    "\n",
    "        cost = T.mean(T.nnet.categorical_crossentropy(calculation, y_output)) + decay*T.sum(sum_sqr_weights)\n",
    "        params = list(weights+biases)\n",
    "        updates = self.sgd(cost=cost, params=params)\n",
    "\n",
    "        self.computation = theano.function(\n",
    "            inputs=[x_input, y_output],\n",
    "            updates=updates,\n",
    "            outputs=cost\n",
    "        )\n",
    "\n",
    "        self.prediction = theano.function(\n",
    "            inputs=[x_input],\n",
    "            outputs=y_prediction\n",
    "        )\n",
    "\n",
    "        return\n",
    "\n",
    "    def init_bias(self, n):\n",
    "        return theano.shared(np.zeros(n), theano.config.floatX)\n",
    "\n",
    "    def init_weight(self, n_in, n_out, is_logistic_function=False):\n",
    "\n",
    "        weight = np.random.uniform(\n",
    "            size=(n_in, n_out),\n",
    "            low=-np.sqrt(6. / (n_in + n_out)),\n",
    "            high=np.sqrt(6. / (n_in + n_out)),\n",
    "        )\n",
    "\n",
    "        if is_logistic_function:\n",
    "            weight = weight*4\n",
    "\n",
    "        return theano.shared(weight, theano.config.floatX)\n",
    "\n",
    "    def sgd(self, cost, params, lr=0.01):\n",
    "\n",
    "        # return list of gradients\n",
    "        grads = T.grad(cost=cost, wrt=params)\n",
    "\n",
    "        updates = []\n",
    "        for p, g in zip(params, grads):\n",
    "            updates.append([p, p - g * lr])\n",
    "        return updates\n",
    "\n",
    "    def start_train(self, test_x, test_y, epochs=1000, batch_size=100):\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            cost = 0\n",
    "            prediction_batch = []\n",
    "\n",
    "            for cnt in range(0, len(self.train_x), batch_size):\n",
    "\n",
    "                end = cnt + batch_size\n",
    "\n",
    "                if end > len(self.train_x):\n",
    "                    end = len(self.train_x)\n",
    "\n",
    "                train_x_batch = self.train_x[cnt:end]\n",
    "                train_y_batch = self.train_y[cnt:end]\n",
    "\n",
    "                cost += self.computation(train_x_batch, train_y_batch)\n",
    "                prediction = self.prediction(self.train_x)\n",
    "                predict_in_percentage = np.mean(np.argmax(self.train_y, axis=1) == prediction)\n",
    "                prediction_batch.append(predict_in_percentage)\n",
    "\n",
    "            # predictions of train data\n",
    "            prediction = np.mean(prediction_batch)\n",
    "\n",
    "            self.train_cost.append(cost)\n",
    "            self.train_prediction.append(prediction)\n",
    "\n",
    "            if i % 5*batch_size == 0 or i == epochs-1:\n",
    "\n",
    "                print ('epoch: %d, train cost: %s, train predictions: %s \\n' % (i, cost, prediction))\n",
    "                self.start_test(test_x=test_x, test_y=test_y)\n",
    "                print('------------------------------------\\n')\n",
    "\n",
    "    def start_test(self, test_x, test_y):\n",
    "\n",
    "        cost = self.computation(test_x, test_y)\n",
    "        prediction = self.prediction(test_x)\n",
    "\n",
    "        self.test_cost.append(cost)\n",
    "        self.test_prediction.append(prediction)\n",
    "\n",
    "        print ('test cost: %s, test predictions: %s \\n' % (cost, np.mean(np.argmax(test_y, axis=1) == prediction)))\n",
    "\n",
    "    def get_train_result(self):\n",
    "\n",
    "        return self.train_cost, self.train_prediction\n",
    "\n",
    "    def get_test_result(self):\n",
    "\n",
    "        return self.test_cost, self.test_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above code is the class to instanstiate the softmax neural network, Below is the implementation of data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataCollector:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_train = pd.read_csv(\"./data/sat_train.txt\", delimiter=' ')\n",
    "        self.df_test = pd.read_csv(\"./data/sat_test.txt\", delimiter=' ')\n",
    "\n",
    "        # change the index\n",
    "        self.df_train.columns = range(self.df_train.shape[1])\n",
    "        self.df_test.columns = range(self.df_test.shape[1])\n",
    "\n",
    "        self.x_train = self.df_train[range(36)]\n",
    "        self.y_train = self.df_train[36]\n",
    "\n",
    "        self.x_test = self.df_test[range(36)]\n",
    "        self.y_test = self.df_test[36]\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_train_data(self):\n",
    "\n",
    "        return self.x_train, self.one_hot_encoding_data(self.y_train)\n",
    "\n",
    "    def get_test_data(self):\n",
    "\n",
    "        return self.x_test, self.one_hot_encoding_data(self.y_test)\n",
    "\n",
    "    def one_hot_encoding_data(self, df, limit_number=6):\n",
    "\n",
    "        # in this case data 6 is missing so, 7 we assume to be 6\n",
    "        df[df == 7] = 6\n",
    "        df_return = np.zeros((df.shape[0], limit_number))\n",
    "        df_return[np.arange(df.shape[0]), df - 1] = 1\n",
    "        return df_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training the simple multi layer softmax neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cost: 1.81570196997, train predictions: 0.233423545332 \n",
      "\n",
      "test cost: 1.80494240925, test predictions: 0.233616808404 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.58030678257, train predictions: 0.452413170952 \n",
      "\n",
      "test cost: 1.59783264093, test predictions: 0.443221610805 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.50772475648, train predictions: 0.554127198917 \n",
      "\n",
      "test cost: 1.52944113557, test predictions: 0.52376188094 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.44530774248, train predictions: 0.536535859269 \n",
      "\n",
      "test cost: 1.46584239605, test predictions: 0.547273636818 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.38753911742, train predictions: 0.5599909788 \n",
      "\n",
      "test cost: 1.40975716199, test predictions: 0.541270635318 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.33654208885, train predictions: 0.596301308074 \n",
      "\n",
      "test cost: 1.36069419182, test predictions: 0.592796398199 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.29282180176, train predictions: 0.593820478124 \n",
      "\n",
      "test cost: 1.31905337287, test predictions: 0.577288644322 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.25607257896, train predictions: 0.587054578259 \n",
      "\n",
      "test cost: 1.28280887488, test predictions: 0.569284642321 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.22441644685, train predictions: 0.584573748309 \n",
      "\n",
      "test cost: 1.2513825361, test predictions: 0.568284142071 \n",
      "\n",
      "------------------------------------\n",
      "\n",
      "train cost: 1.19667554459, train predictions: 0.579612088408 \n",
      "\n",
      "test cost: 1.22371144788, test predictions: 0.570785392696 \n",
      "\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_collector = DataCollector()\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "\n",
    "number_data = train_x.shape[0]\n",
    "\n",
    "softmax_nn = SoftmaxNeuralNetwork(train_x=train_x.as_matrix(), train_y=train_y, list_of_neuron_on_hidden_layer=[10])\n",
    "softmax_nn.start_train(batch_size=number_data, test_x=test_x, test_y=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the optimal batch size for mini-batch gradient descent while training the neural network by evaluating the performances for different batch sizes. Set this as the batch size for the rest of the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the training error and test accuracy against number of epochs for the 3-layer\n",
    "network for each batch size. Limit search space to:{4,8,16,32,64}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the helper class to do plotting stuff, we create another class to do such task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DataVisualization:\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def show_plot(self, list_x_point, list_y_point, x_label, y_label, title, figure_name):\n",
    "        plt.figure()\n",
    "        plt.plot(list_x_point, list_y_point)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(title)\n",
    "        plt.savefig(figure_name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.Plot the training error and test accuracy against number of epochs for the 3-layer\n",
    "network for each batch size. Limit search space to:{4,8,16,32,64}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train with batch size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollector()\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "\n",
    "# number_data = train_x.shape[0]\n",
    "number_data = 4\n",
    "\n",
    "number_epoch = 100\n",
    "\n",
    "softmax_nn = SoftmaxNeuralNetwork(train_x=train_x.as_matrix(), train_y=train_y, list_of_neuron_on_hidden_layer=[10])\n",
    "softmax_nn.start_train(batch_size=number_data, test_x=test_x, test_y=test_y, epochs=number_epoch)\n",
    "\n",
    "cost_train, prediction_train = softmax_nn.get_train_result()\n",
    "cost_test, prediction_test = softmax_nn.get_test_result()\n",
    "\n",
    "%matplotlib inline\n",
    "# visualize\n",
    "data_visualization = DataVisualization()\n",
    "data_visualization.show_plot(\n",
    "    list_x_point=range(number_epoch), list_y_point=cost_train,\n",
    "    x_label=\"epochs\", y_label=\"costs\", title=\"Cross Entropy\", figure_name=\"cross_entropy_cost.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with 8 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollector()\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "\n",
    "# number_data = train_x.shape[0]\n",
    "number_data = 8\n",
    "\n",
    "number_epoch = 100\n",
    "\n",
    "softmax_nn = SoftmaxNeuralNetwork(train_x=train_x.as_matrix(), train_y=train_y, list_of_neuron_on_hidden_layer=[10])\n",
    "softmax_nn.start_train(batch_size=number_data, test_x=test_x, test_y=test_y, epochs=number_epoch)\n",
    "\n",
    "cost_train, prediction_train = softmax_nn.get_train_result()\n",
    "cost_test, prediction_test = softmax_nn.get_test_result()\n",
    "\n",
    "%matplotlib inline\n",
    "# visualize\n",
    "data_visualization = DataVisualization()\n",
    "data_visualization.show_plot(\n",
    "    list_x_point=range(number_epoch), list_y_point=cost_train,\n",
    "    x_label=\"epochs\", y_label=\"costs\", title=\"Cross Entropy\", figure_name=\"cross_entropy_cost.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with 16 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollector()\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "\n",
    "# number_data = train_x.shape[0]\n",
    "number_data = 16\n",
    "\n",
    "number_epoch = 100\n",
    "\n",
    "softmax_nn = SoftmaxNeuralNetwork(train_x=train_x.as_matrix(), train_y=train_y, list_of_neuron_on_hidden_layer=[10])\n",
    "softmax_nn.start_train(batch_size=number_data, test_x=test_x, test_y=test_y, epochs=number_epoch)\n",
    "\n",
    "cost_train, prediction_train = softmax_nn.get_train_result()\n",
    "cost_test, prediction_test = softmax_nn.get_test_result()\n",
    "\n",
    "%matplotlib inline\n",
    "# visualize\n",
    "data_visualization = DataVisualization()\n",
    "data_visualization.show_plot(\n",
    "    list_x_point=range(number_epoch), list_y_point=cost_train,\n",
    "    x_label=\"epochs\", y_label=\"costs\", title=\"Cross Entropy\", figure_name=\"cross_entropy_cost.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with 32 batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollector()\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "\n",
    "# number_data = train_x.shape[0]\n",
    "number_data = 32\n",
    "\n",
    "number_epoch = 100\n",
    "\n",
    "softmax_nn = SoftmaxNeuralNetwork(train_x=train_x.as_matrix(), train_y=train_y, list_of_neuron_on_hidden_layer=[10])\n",
    "softmax_nn.start_train(batch_size=number_data, test_x=test_x, test_y=test_y, epochs=number_epoch)\n",
    "\n",
    "cost_train, prediction_train = softmax_nn.get_train_result()\n",
    "cost_test, prediction_test = softmax_nn.get_test_result()\n",
    "\n",
    "%matplotlib inline\n",
    "# visualize\n",
    "data_visualization = DataVisualization()\n",
    "data_visualization.show_plot(\n",
    "    list_x_point=range(number_epoch), list_y_point=cost_train,\n",
    "    x_label=\"epochs\", y_label=\"costs\", title=\"Cross Entropy\", figure_name=\"cross_entropy_cost.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with 64 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollector()\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "\n",
    "# number_data = train_x.shape[0]\n",
    "number_data = 64\n",
    "\n",
    "number_epoch = 100\n",
    "\n",
    "softmax_nn = SoftmaxNeuralNetwork(train_x=train_x.as_matrix(), train_y=train_y, list_of_neuron_on_hidden_layer=[10])\n",
    "softmax_nn.start_train(batch_size=number_data, test_x=test_x, test_y=test_y, epochs=number_epoch)\n",
    "\n",
    "cost_train, prediction_train = softmax_nn.get_train_result()\n",
    "cost_test, prediction_test = softmax_nn.get_test_result()\n",
    "\n",
    "%matplotlib inline\n",
    "# visualize\n",
    "data_visualization = DataVisualization()\n",
    "data_visualization.show_plot(\n",
    "    list_x_point=range(number_epoch), list_y_point=cost_train,\n",
    "    x_label=\"epochs\", y_label=\"costs\", title=\"Cross Entropy\", figure_name=\"cross_entropy_cost.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
